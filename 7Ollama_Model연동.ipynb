{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, so I'm trying to understand what LangChain is. From the information \"\n",
      " \"given, it seems like it's a module related to language models in Python. The \"\n",
      " 'user provided some code examples where they import LangChain along with an '\n",
      " 'instance of the GPT-3 model. \\n'\n",
      " '\\n'\n",
      " 'First, I know that in Python, modules are used to package software together, '\n",
      " 'so LangChain must have functions and classes that can be imported and used '\n",
      " 'to interact with language models. The example shows importing from '\n",
      " '\"langchain.\"langchain,\" which suggests that it\\'s a third-party library.\\n'\n",
      " '\\n'\n",
      " 'I remember reading about libraries like PyTorch, TensorFlow, and others that '\n",
      " 'provide tools for working with language models. Maybe LangChain is similar '\n",
      " 'but tailored specifically for the PyTorch framework? That would make sense '\n",
      " 'because PyTorch is widely used in machine learning and deep learning '\n",
      " 'projects involving neural networks.\\n'\n",
      " '\\n'\n",
      " 'So, I think LangChain might be a library that provides functions to work '\n",
      " \"with a model called Llama. Since it's from LangChain, perhaps it offers \"\n",
      " 'utilities like initializing the model with specific hyperparameters or '\n",
      " 'methods for interacting with the model. \\n'\n",
      " '\\n'\n",
      " 'Looking at the examples provided:\\n'\n",
      " '- From LangChain import Llama.\\n'\n",
      " '- Then an instance of GPT-3 is imported.\\n'\n",
      " '\\n'\n",
      " 'I can see that maybe there are functions in LangChain to get the model and '\n",
      " 'then create a GPT-3 instance using it. So, perhaps LangChain helps in '\n",
      " 'setting up the model for tasks like generating text or answering questions '\n",
      " 'using the GPT-3 architecture.\\n'\n",
      " '\\n'\n",
      " \"Wait, but isn't the user suggesting that the example might be incomplete? If \"\n",
      " 'I run it as given, would it work? Let me try to think about what could go '\n",
      " \"wrong. If the module isn't installed, or if there's an import error, it \"\n",
      " \"wouldn't function properly. So, maybe LangChain is meant to be used \"\n",
      " 'alongside a specific model like Llama, which might require additional setup '\n",
      " 'beyond just importing the module.\\n'\n",
      " '\\n'\n",
      " \"I also wonder about the purpose of LangChain itself. It's supposed to \"\n",
      " 'provide tools for working with language models. So, perhaps it includes '\n",
      " 'functions that make interacting with these models easier, like initializing '\n",
      " 'them, configuring their parameters, or even using them in a more integrated '\n",
      " 'system.\\n'\n",
      " '\\n'\n",
      " 'In terms of structure, I might need to look into how such modules are '\n",
      " 'typically organized. They often have a class or function that handles the '\n",
      " \"creation and configuration of the model. For instance, maybe there's a \"\n",
      " 'function called create_model() which takes hyperparameters as arguments and '\n",
      " 'returns an instance of the model.\\n'\n",
      " '\\n'\n",
      " 'Additionally, using Llama in PyTorch would require setting up some '\n",
      " 'environment variables, like the path to the Llama model file. So, LangChain '\n",
      " 'might handle that setup automatically or provide options for users to '\n",
      " 'configure it without modifying the source code directly.\\n'\n",
      " '\\n'\n",
      " \"I'm also curious about how LangChain interacts with other libraries. For \"\n",
      " 'example, if I have another module that uses a language model, would '\n",
      " 'importing LangChain help bridge the gap between them? Maybe through some '\n",
      " 'compatibility layer or shared functions.\\n'\n",
      " '\\n'\n",
      " 'To summarize my thoughts:\\n'\n",
      " '1. LangChain is a Python library for working with language models.\\n'\n",
      " '2. It likely provides functions to initialize and configure models like '\n",
      " 'Llama within PyTorch.\\n'\n",
      " '3. Examples show initializing a model and creating a GPT-3 instance, '\n",
      " \"suggesting it's used in tasks involving text generation or question \"\n",
      " 'answering.\\n'\n",
      " '\\n'\n",
      " 'I think I should look into how LangChain is structured and what specific '\n",
      " 'functions are available for more detailed exploration. Maybe starting by '\n",
      " 'importing the necessary modules and exploring their documentation to see if '\n",
      " 'there are any core functions related to model initialization or usage.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a Python module designed for working with language models, '\n",
      " 'particularly within the PyTorch framework. It provides utilities to '\n",
      " 'initialize and configure models like Llama and includes functions for tasks '\n",
      " 'such as text generation and question answering.\\n'\n",
      " '\\n'\n",
      " '**Key Points:**\\n'\n",
      " '\\n'\n",
      " '1. **Purpose**: LangChain facilitates interactions with language models '\n",
      " 'using PyTorch, enabling tasks like text generation and question answering.\\n'\n",
      " '2. **Structure**: It likely contains functions to create model instances '\n",
      " 'with specified hyperparameters and methods for integrating these models into '\n",
      " 'larger systems.\\n'\n",
      " '3. **Examples**: The provided code demonstrates importing from '\n",
      " '\"langchain.\"langchain\" and creating a GPT-3 instance using an Llama model.\\n'\n",
      " '\\n'\n",
      " '**Conclusion:**\\n'\n",
      " 'LangChain is useful for researchers and developers working with language '\n",
      " 'models, offering tools to set up models within PyTorch and facilitate tasks '\n",
      " 'involving text generation and question answering.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 deepseek-r1:1.5b 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basics of Python.\n",
      "\n",
      "Python is a programming language. It's known for being easy to learn and powerful for various tasks. I should mention that it's open-source and has a large community. Maybe talk about its use cases like web development, data analysis, machine learning, and more.\n",
      "\n",
      "I should explain that it's interpreted, which means the code is executed line by line. Also, mention that it uses a simple syntax with readable code. Highlight that it's versatile and has a lot of libraries and frameworks.\n",
      "\n",
      "Wait, the user might be a beginner, so I need to keep the explanation straightforward. Avoid technical jargon unless necessary. Make sure to define key terms like \"programming language\" and \"open-source.\"\n",
      "\n",
      "Maybe include a simple example, like \"print('Hello, World!')\", to show how it's used. But since the answer is supposed to be concise, maybe keep it to the basics without an example unless the user asks for it.\n",
      "\n",
      "Check if there's any important feature I missed. Oh, it's also used in scientific computing with NumPy and pandas, and in automation. But maybe that's too detailed. The user might not need that unless specified.\n",
      "\n",
      "So, the answer should be clear, concise, and cover the main points: what it is, its features, use cases, and maybe a brief mention of its community and ease of use. Make sure to use Korean terms and avoid any markdown.\n",
      "</think>\n",
      "\n",
      "파이썬은 **프로그래밍 언어**입니다. 쉽게 배울 수 있으며, 다양한 분야에서 활용할 수 있는 강력한 언어입니다. 예를 들어, 웹 개발, 데이터 분석, 머신러닝, 과학 계산 등 다양한 분야에서 사용됩니다. 파이썬은 **열린 소스**로, 개발자들이 자유롭게 사용하고 라이브러리(예: NumPy, pandas, Django 등)를 활용할 수 있습니다. 코드의 구문은 간결하고 읽기 쉽습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 qwen2.5:1.5b 모델 로드\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한글로 답변해 줘\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I will compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "Next, I'll look at the decimal parts. The first number has 0.9, while the second has 0.11.\n",
      "\n",
      "Since 0.9 is greater than 0.11, it follows that 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Number Parts\n",
      "Both numbers have the same whole number part, which is:\n",
      "\\[\n",
      "9\n",
      "\\]\n",
      "\n",
      "Since the whole number parts are equal, we need to compare the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Decimal Parts\n",
      "- **9.9** can be written as **9.90**\n",
      "- **9.11** remains as **9.11**\n",
      "\n",
      "Now, let's look at each digit after the decimal point:\n",
      "- **Tenths place:**  \n",
      "  - **9.90** has a **9** in the tenths place.\n",
      "  - **9.11** has a **1** in the tenths place.\n",
      "\n",
      "Since **9** is greater than **1**, **9.90** (which is equivalent to **9.9**) is larger than **9.11**.\n",
      "\n",
      "### Conclusion\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger than } 9.11}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "174fec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I will compare the two numbers: 9.9 and 9.11.\n",
       "\n",
       "Both numbers have the same whole number part, which is 9.\n",
       "\n",
       "Next, I'll look at the decimal parts. The first number has 0.9, while the second has 0.11.\n",
       "\n",
       "Since 0.9 is greater than 0.11, it follows that 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Number Parts\n",
       "Both numbers have the same whole number part, which is:\n",
       "\\[\n",
       "9\n",
       "\\]\n",
       "\n",
       "Since the whole number parts are equal, we need to compare the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Decimal Parts\n",
       "- **9.9** can be written as **9.90**\n",
       "- **9.11** remains as **9.11**\n",
       "\n",
       "Now, let's look at each digit after the decimal point:\n",
       "- **Tenths place:**  \n",
       "  - **9.90** has a **9** in the tenths place.\n",
       "  - **9.11** has a **1** in the tenths place.\n",
       "\n",
       "Since **9** is greater than **1**, **9.90** (which is equivalent to **9.9**) is larger than **9.11**.\n",
       "\n",
       "### Conclusion\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think about this step by step. \n",
      "\n",
      "First, I need to compare the two numbers. Both are decimal numbers. Let me write them down again to visualize them better: 9.9 and 9.11. \n",
      "\n",
      "Starting with the whole number part. Both numbers have a 9 in the units place. So, the whole numbers are equal. That means I need to look at the decimal parts to determine which is larger.\n",
      "\n",
      "Now, looking at the decimal parts. The first number is 9.9, which is the same as 9.90. The second number is 9.11. So, comparing the tenths place first. The first number has 9 in the tenths place, and the second number has 1 in the tenths place. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \n",
      "\n",
      "Wait, let me double-check. If I write them out with the same number of decimal places, 9.9 is 9.90 and 9.11 is 9.11. Comparing the tenths digit: 9 vs 1. Yep, 9 is bigger. So even though the second number has more decimal places, the tenths place is smaller. Therefore, 9.9 is larger.\n",
      "\n",
      "I think that's right. Another way to look at it is to subtract them. 9.9 minus 9.11 equals 0.79. Since the result is positive, that means 9.9 is larger. \n",
      "\n",
      "No, wait, actually, 9.9 minus 9.11 is 0.79? Let me calculate that again. 9.9 minus 9.11. If I subtract 9.11 from 9.9, it's like subtracting 9.11 from 9.90. So, 9.90 - 9.11 = 0.79. Yes, that's correct. So the difference is positive, meaning 9.9 is larger. \n",
      "\n",
      "I don't think there's any other way to interpret this. The numbers are straightforward. The whole numbers are the same, but the decimal parts differ. The tenths place of 9.9 is 9, which is higher than 1 in 9.11. So, 9.9 is bigger. \n",
      "\n",
      "I guess that's the answer. 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "\n",
      "**이유:**  \n",
      "1. **정수 부분:** 9.9와 9.11 모두 9를 가진다.  \n",
      "2. **소수 부분:**  \n",
      "   - 9.9는 9.90으로, 소수 부분의 첫 번째 자리가 **9**이다.  \n",
      "   - 9.11은 소수 부분의 첫 번째 자리가 **1**이다.  \n",
      "   - 9 > 1이므로, 9.9는 9.11보다 더 큰 수이다.  \n",
      "\n",
      "따라서, **9.9 > 9.11**입니다."
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3623342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think about this step by step. \n",
       "\n",
       "First, I need to compare the two numbers. Both are decimal numbers. Let me write them down again to visualize them better: 9.9 and 9.11. \n",
       "\n",
       "Starting with the whole number part. Both numbers have a 9 in the units place. So, the whole numbers are equal. That means I need to look at the decimal parts to determine which is larger.\n",
       "\n",
       "Now, looking at the decimal parts. The first number is 9.9, which is the same as 9.90. The second number is 9.11. So, comparing the tenths place first. The first number has 9 in the tenths place, and the second number has 1 in the tenths place. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \n",
       "\n",
       "Wait, let me double-check. If I write them out with the same number of decimal places, 9.9 is 9.90 and 9.11 is 9.11. Comparing the tenths digit: 9 vs 1. Yep, 9 is bigger. So even though the second number has more decimal places, the tenths place is smaller. Therefore, 9.9 is larger.\n",
       "\n",
       "I think that's right. Another way to look at it is to subtract them. 9.9 minus 9.11 equals 0.79. Since the result is positive, that means 9.9 is larger. \n",
       "\n",
       "No, wait, actually, 9.9 minus 9.11 is 0.79? Let me calculate that again. 9.9 minus 9.11. If I subtract 9.11 from 9.9, it's like subtracting 9.11 from 9.90. So, 9.90 - 9.11 = 0.79. Yes, that's correct. So the difference is positive, meaning 9.9 is larger. \n",
       "\n",
       "I don't think there's any other way to interpret this. The numbers are straightforward. The whole numbers are the same, but the decimal parts differ. The tenths place of 9.9 is 9, which is higher than 1 in 9.11. So, 9.9 is bigger. \n",
       "\n",
       "I guess that's the answer. 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
       "\n",
       "**이유:**  \n",
       "1. **정수 부분:** 9.9와 9.11 모두 9를 가진다.  \n",
       "2. **소수 부분:**  \n",
       "   - 9.9는 9.90으로, 소수 부분의 첫 번째 자리가 **9**이다.  \n",
       "   - 9.11은 소수 부분의 첫 번째 자리가 **1**이다.  \n",
       "   - 9 > 1이므로, 9.9는 9.11보다 더 큰 수이다.  \n",
       "\n",
       "따라서, **9.9 > 9.11**입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4cdc8",
   "metadata": {},
   "source": [
    "### LangGraph를 사용하여 DeepSeek 모델(추론)과 Qwen 모델(한글응답)을 연동하기\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f146748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n",
      "<think>\n",
      "Okay, the user is asking which number is larger between 9.9 and 9.11. Let me think through this step by step.\n",
      "\n",
      "First, I need to compare the two numbers. Both are decimals, so I can start by looking at the whole numbers. Both are 9, so that's equal. Now, the decimal parts matter here. \n",
      "\n",
      "The first number is 9.9, which is the same as 9.90 when converted to two decimal places. The second number is 9.11. Comparing the tenths place: 9 vs. 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place already shows that 9.9 is larger. \n",
      "\n",
      "So, even though 9.11 has two decimal places, the tenths place is 1, which is less than 9. Therefore, 9.9 is larger. \n",
      "\n",
      "I should make sure to explain that by aligning the decimals, the comparison becomes straightforward. The key point is that the tenths place in 9.9 is 9, which is greater than 1 in 9.11, making 9.9 larger. \n",
      "\n",
      "I need to present this clearly without using technical terms, keeping it conversational. Highlight the importance of comparing each digit starting from the left, and how the tenths place determines the result here.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 숫자는 **9.9**입니다.  \n",
      "두 숫자는 자리수를 바탕으로 비교하면 모두 9를 가진다며, 그 다음 자리수에서 9.9는 1을 가진 9.11보다 9가 더 큰 숫자입니다. 따라서 9.9가 더 큰 숫자입니다.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nTo make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\\n\\nNow that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\\n\\nStarting with the units place: Both numbers have a 9 in the units place, so they are equal there.\\n\\nNext, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, the user is asking which number is larger between 9.9 and 9.11. Let me think through this step by step.\\n\\nFirst, I need to compare the two numbers. Both are decimals, so I can start by looking at the whole numbers. Both are 9, so that's equal. Now, the decimal parts matter here. \\n\\nThe first number is 9.9, which is the same as 9.90 when converted to two decimal places. The second number is 9.11. Comparing the tenths place: 9 vs. 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place already shows that 9.9 is larger. \\n\\nSo, even though 9.11 has two decimal places, the tenths place is 1, which is less than 9. Therefore, 9.9 is larger. \\n\\nI should make sure to explain that by aligning the decimals, the comparison becomes straightforward. The key point is that the tenths place in 9.9 is 9, which is greater than 1 in 9.11, making 9.9 larger. \\n\\nI need to present this clearly without using technical terms, keeping it conversational. Highlight the importance of comparing each digit starting from the left, and how the tenths place determines the result here.\\n</think>\\n\\n9.9와 9.11 중 더 큰 숫자는 **9.9**입니다.  \\n두 숫자는 자리수를 바탕으로 비교하면 모두 9를 가진다며, 그 다음 자리수에서 9.9는 1을 가진 9.11보다 9가 더 큰 숫자입니다. 따라서 9.9가 더 큰 숫자입니다.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, the user is asking which number is larger between 9.9 and 9.11. Let me think through this step by step.\n",
      "\n",
      "First, I need to compare the two numbers. Both are decimals, so I can start by looking at the whole numbers. Both are 9, so that's equal. Now, the decimal parts matter here. \n",
      "\n",
      "The first number is 9.9, which is the same as 9.90 when converted to two decimal places. The second number is 9.11. Comparing the tenths place: 9 vs. 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place already shows that 9.9 is larger. \n",
      "\n",
      "So, even though 9.11 has two decimal places, the tenths place is 1, which is less than 9. Therefore, 9.9 is larger. \n",
      "\n",
      "I should make sure to explain that by aligning the decimals, the comparison becomes straightforward. The key point is that the tenths place in 9.9 is 9, which is greater than 1 in 9.11, making 9.9 larger. \n",
      "\n",
      "I need to present this clearly without using technical terms, keeping it conversational. Highlight the importance of comparing each digit starting from the left, and how the tenths place determines the result here.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 숫자는 **9.9**입니다.  \n",
      "두 숫자는 자리수를 바탕으로 비교하면 모두 9를 가진다며, 그 다음 자리수에서 9.9는 1을 가진 9.11보다 9가 더 큰 숫자입니다. 따라서 9.9가 더 큰 숫자입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 추론 모델\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#응답 모델 (한글처리 가능)\n",
    "#generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "#qwen3:1.7b\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a6834b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9a58b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit from left to right:\n",
      "\n",
      "- The units place for both numbers is 9.\n",
      "- In the tenths place, 9 has a 9 and 9.11 has a 1.\n",
      "  \n",
      "Since 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But they have different decimal parts. I need to figure out which decimal is larger.\n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, so that's not the difference. Now, looking at the tenths place. The first number, 9.9, has a 9 in the tenths place. The second one, 9.11, has a 1 in the tenths place. Wait, but 9.11 has two decimal places. So maybe I should consider the next digit?\n",
      "\n",
      "Wait, no. The tenths place is the first decimal digit. So 9.9 is 9.90 when written with two decimal places. Then comparing 9.90 and 9.11. The tenths place is 9 vs 1. Since 9 is greater than 1, 9.90 is larger. So 9.9 is bigger than 9.11. Got it. The key was aligning the decimals to compare each digit step by step.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "두 수는 정수 부분(9)이 같으므로, 소수 부분을 비교해야 합니다.  \n",
      "9.9는 소수 부분 9.0으로, 9.11은 소수 부분 1.1으로 나타냅니다.  \n",
      "소수 부분에서 9 > 1이므로, 9.9는 9.11보다 더 큰 수입니다.<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are the same. But they have different decimal parts. I need to figure out which decimal is larger.\n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, so that's not the difference. Now, looking at the tenths place. The first number, 9.9, has a 9 in the tenths place. The second one, 9.11, has a 1 in the tenths place. Wait, but 9.11 has two decimal places. So maybe I should consider the next digit?\n",
      "\n",
      "Wait, no. The tenths place is the first decimal digit. So 9.9 is 9.90 when written with two decimal places. Then comparing 9.90 and 9.11. The tenths place is 9 vs 1. Since 9 is greater than 1, 9.90 is larger. So 9.9 is bigger than 9.11. Got it. The key was aligning the decimals to compare each digit step by step.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "두 수는 정수 부분(9)이 같으므로, 소수 부분을 비교해야 합니다.  \n",
      "9.9는 소수 부분 9.0으로, 9.11은 소수 부분 1.1으로 나타냅니다.  \n",
      "소수 부분에서 9 > 1이므로, 9.9는 9.11보다 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cec5a4",
   "metadata": {},
   "source": [
    "### 2개의 모델을 연동한 코드를 Gradio 를 사용하여 UI로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0f03026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    #model=\"qwen3:1.7b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    #thinking_content = ensure_utf8_string(response.content)\n",
    "    thinking_content = response.content\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    # question = ensure_utf8_string(state[\"question\"])\n",
    "    # thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    thinking = state[\"thinking\"]\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    #answer_content = ensure_utf8_string(response.content)\n",
    "    answer_content = response.content\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    #launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
